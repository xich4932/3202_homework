{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSCI 3202, Fall 2021\n",
    "\n",
    "# November 8, 2021\n",
    "\n",
    "# Lecture 31:  Markov Decision Processes (MDP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose there is a betting game that has these rules. \n",
    "\n",
    "- You start with some amount of money.\n",
    "- You can wager any amount in increments of 1, with the maximum amount permitted = however much money you currently have.\n",
    "- You flip a fair coin. If heads, you win your wager and your money increases by that amount. If tails, you lose your wager and your money decreases by that amount.\n",
    "- You start playing this game with 5. You will quit if you have at least 10 at any point, or if you lose all of your money.\n",
    "\n",
    "In typical American-style roulette, there are 18 red and 18 black bins, but also 2 green ones (0 and 00). Thus, the odds of winning a color-based bet are $ 18/38 \\approx 0.473 $. So this coin-flipping game is like version of roulette."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A:** We will approach the problem of figuring out how to play this game by using **Markov Decision Processes**, which include the idea of **risk versus reward**.\n",
    "\n",
    "Create a list of the possible states given these conditions:\n",
    "\n",
    "1. The rules of the game dictate that you can never have negative money, so the lowest state is $s=0$. What about the upper limit? You will quit if you have more than 10. But what if you had 9, and then won your bet?\n",
    "\n",
    "2. We will also create a list of all the terminal states. These include 0 and any states with value 10 or greater."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = list(range(0, 19))\n",
    "terminal_states = [0]+list(range(10,19))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B:** \n",
    "1. Define an `actions` function that takes an argument `s` for the current state, and returns a list of all the possible actions - or wager amounts - that we can make. You may assume that `terminal_states` is implicitly known, so does not need to be fed in as an argument, and that `[None]` is the appropriate set of actions available to a terminal state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actions(s):\n",
    "    \n",
    "    if s in terminal_states:\n",
    "        return [None]\n",
    "    else:\n",
    "        return list(range(1,s+1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Define a `transition` function that takes arguments `s` for the current states and `a` for the current action and returns a list of tuples. The second element of each tuple is an \"adjacent\" state that can be reached from `s` by action `a`, and the first element is the probability of this transition occurring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition(s,a):\n",
    "    \n",
    "    if a is None:\n",
    "        return [(0, s)]\n",
    "    \n",
    "    else:\n",
    "        return [(0.5, s+a), (0.5, s-a)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Define a `reward` function that takes an argument of `s` for the current state, and returns the reward for that state.\n",
    "\n",
    "First, you'll need to choose the rewards for the terminal states.\n",
    "\n",
    "- If you win \\\\$10 or more, then a natural choice is to return the amount of money that you have won. For example, `reward(11)=11`\n",
    "- If you lose all \\\\$5 that you started with, then you might think to return 0 as the reward. But really, you have actually lost \\\\$5, so a better choice of reward is -5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we need to choose the default reward for being in a non-terminal state. The typical thing to do is incorporate a small negative reward, so that the agent is incentivized to find a winning state. To start, let's use -0.01 as a reward for being in a non-terminal state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward(s):\n",
    "    if s == 0:\n",
    "        return -5\n",
    "    elif s >= 10:\n",
    "        return s-5\n",
    "    else:\n",
    "        return 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C:** Let's code up **value iteration** to solve for the utilities of each state under an optimal policy. Use a discount factor of 0.999 to reflect the fact that we can play many iterations of this game quickly, so future rewards are not really all that far away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's walk through a single iteration of value iteration before we tackle the real deal. First, we must initialize something to track the largest change in utility of states this iteration, and an initial `utility` for all states. We can use a dictionary for `utility`, to map states (keys) to their utility (values). Here, we initialize all states to have 0 utility initially. In the end, it won't matter what value we used for initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_change = 0\n",
    "utility_old = {s: 0 for s in states}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to iterate over all states `s`, but let's start by just calculating the maximum expected utility for just one state. Then, we can add a loop around our code. Arbitrarily, let's say we start with `s=2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each available action from state `s`, we need to know what states are possible to reach and what their probabilities are. We need output from `transition(s,a)` for each possible action from state `s`.\n",
    "\n",
    "Here is one way we could get the set of next states and their probabilities, just for the action of betting 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_states = transition(s, actions(s)[0])\n",
    "print(next_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want all of the next states and their probabilities under all of the actions possible from `s`, then we could use the following list comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_states = [transition(s,a) for a in actions(s)]\n",
    "print(next_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`next_states` is a list-of-lists, where the first list corresponds to the original output that we had from our simpler call to `transition`, and the second list corresponds to the output of `transition(s,actions(s)[1])`.\n",
    "\n",
    "Now we need to calculate the maximum expected utility among all the possible actions. We can initialize a new utility dictionary to hold the updated utilities for each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utility_new = utility_old.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can implement a nice little loop to find the best utility among the possible actions from state `s`. Complete the codes below to calculate the expected utility (`newsum`) for each set of probabilities and states in `next_state`. The use the Bellman Equation to calculate `utility_new[s]` and update `max_change`, if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_utility = -999\n",
    "\n",
    "# Since next_state is a list of lists, we need to loop through\n",
    "# each list\n",
    "for k in range(len(next_states)):\n",
    "    newsum = 0\n",
    "    # Within each list, we are listing more things, let's loop through those too\n",
    "    for j in range(len(next_states[k])):\n",
    "        \n",
    "        prob_s_prime_given_s_a = next_states[k][j][0]\n",
    "        utility = utility_old[next_states[k][j][1]]\n",
    "        newsum = newsum + prob_s_prime_given_s_a*utility\n",
    "    \n",
    "    # Choose the maximum sum. This has a chance to be updated each time\n",
    "    # the outer k loop is iterated through.\n",
    "    best_utility = max(best_utility, newsum)\n",
    "\n",
    "# Bellman update\n",
    "utility_new[s] = reward(s)+gamma*best_utility\n",
    "max_change = max(max_change, abs(utility_new[s]-utility_old[s]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(utility_new[s])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is gently modified pseudocode from the lecture slides. We now have expressions for most of the things we need to fill in now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we actually iterate, we will need some convergence/exit criterion. That's where the tolerance `tol` and Equation 17.8 from the textbook come into play. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{If}~~ |U_{i+1} - U_i| < \\texttt{tol}\\cdot \\dfrac{1-\\gamma}{\\gamma},~~\\text{then}~~ |U_{i+1} - U| < \\texttt{tol}$$\n",
    "\n",
    "What this tells us in plain English is that if some measure of difference between utilities on iteration $i$ and $i+1$ is less than $\\text{tol}\\cdot \\dfrac{1-\\gamma}{\\gamma}$, then the error in the utility estimate in iteration $i+1$ is less than $\\text{tol}$ itself.\n",
    "\n",
    "Let's use an absolute error tolerance in utility of 0.001, and we can use `max_change` that we have been tracking as an estimate of $|U_{i+1} - U_i|$.  So we should exit our main iteration loop if $\\texttt{max_change} <  \\texttt{tol}\\cdot \\dfrac{1-\\gamma}{\\gamma}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALUE ITERATION:\n",
    "\n",
    "gamma = 0.999\n",
    "tol = 0.001\n",
    "\n",
    "# initilize utility for all states\n",
    "\n",
    "# iterate:\n",
    "\n",
    "    # make a copy of current utility estimate, to be modified\n",
    "\n",
    "    # initialize maximum change to 0\n",
    "\n",
    "    # for each state s:\n",
    "\n",
    "        # for each available action, what next states\n",
    "        # are possible, and their probabilities?\n",
    "\n",
    "        # calculate the maximum expected utility\n",
    "\n",
    "        # new utility of s = reward(s) + \n",
    "        #                    discounted max expected utility\n",
    "\n",
    "        # update maximum change in utilities, if needed\n",
    "\n",
    "    # if maximum change in utility from one iteration to the\n",
    "    # next is less than some tolerance, break!\n",
    "        \n",
    "# upon exit, utility_new is the utility of each state under an optimal policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALUE ITERATION:\n",
    "\n",
    "gamma = 0.999\n",
    "tol = 0.001\n",
    "\n",
    "# initilize utility for all states\n",
    "\n",
    "# iterate:\n",
    "\n",
    "    # make a copy of current utility estimate, to be modified\n",
    "\n",
    "    # initialize maximum change to 0\n",
    "\n",
    "    # for each state s:\n",
    "\n",
    "        # for each available action, what next states\n",
    "        # are possible, and their probabilities?\n",
    "\n",
    "        # calculate the maximum expected utility\n",
    "\n",
    "        # new utility of s = reward(s) + \n",
    "        #                    discounted max expected utility\n",
    "\n",
    "        # update maximum change in utilities, if needed\n",
    "\n",
    "    # if maximum change in utility from one iteration to the\n",
    "    # next is less than some tolerance, break!\n",
    "        \n",
    "# upon exit, utility_new is the utility of each state under an optimal policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution:\n",
    "\n",
    "# VALUE ITERATION:\n",
    "df = 0.999\n",
    "tol = 0.001\n",
    "\n",
    "# initilize utility for all states\n",
    "utility_new = {s : 0 for s in states}\n",
    "\n",
    "# iterate:\n",
    "while True:\n",
    "\n",
    "    # make a copy of current utility estimate, to be modified\n",
    "    utility_old = utility_new.copy()\n",
    "\n",
    "    # initialize maximum change to 0\n",
    "    max_change = 0\n",
    "\n",
    "    # for each state s:\n",
    "    for s in states:\n",
    "\n",
    "        # for each available action, what next states\n",
    "        # are possible, and their probabilities?\n",
    "        next_states = [transition(s, a) for a in actions(s)]\n",
    "\n",
    "        # calculate the maximum expected utility\n",
    "        best_utility = -999\n",
    "        for k in range(len(next_states)):\n",
    "            newsum = sum([next_states[k][j][0]*utility_old[next_states[k][j][1]] for j in range(len(next_states[k]))])\n",
    "            best_utility = max(best_utility, newsum)\n",
    "            if len(next_states)==1:\n",
    "                best_utility = newsum\n",
    "        \n",
    "        # new utility of s = reward(s) + \n",
    "        #                    discounted max expected utility\n",
    "        utility_new[s] = reward(s) + df*best_utility\n",
    "\n",
    "        # update maximum change in utilities, if needed\n",
    "        max_change = max(max_change, abs(utility_new[s]-utility_old[s]))\n",
    "\n",
    "    # if maximum change in utility from one iteration to the\n",
    "    # next is less than some tolerance, break!\n",
    "    if (df==1 and max_change < tol) or max_change < tol*(1-df)/df:\n",
    "        break\n",
    "\n",
    "# upon exit, utility_new is the utility of each state under an optimal policy\n",
    "#utility_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to compare the expected utility of all of the different actions available, so that we can choose the best one for each state:\n",
    "\n",
    "$$\\pi^*(s) = \\underset{a \\in \\text{actions}(s)}{\\arg\\max} \\sum_{s'} P(s' \\mid s,a) U(s')$$\n",
    "\n",
    "where $P$ we can get from the `transition` function, and $U$ is the utility we just found from value iteration.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the policy for each state\n",
    "policy = {s : None for s in states}\n",
    "\n",
    "# loop over states to find the action that maximizes expected utility\n",
    "for s in states:\n",
    "    \n",
    "    # initialize the best utility to something very bad, so we can improve it\n",
    "    best_utility = (-999, None)\n",
    "    \n",
    "    # loop over actions, find which gives the highest expected utility\n",
    "    for a in actions(s):\n",
    "        \n",
    "        # calculate the expected utility of action a from state s\n",
    "        newsum = sum([p*utility_new[s2] for p, s2 in transition(s,a)])\n",
    "        \n",
    "        # if this action has higher expected utility than the current best,\n",
    "        # replace the best (utility, action) tuple with this one\n",
    "        if newsum > best_utility[0]:\n",
    "            best_utility = (newsum, a)\n",
    "            \n",
    "    # now we have the action (second element) that leads\n",
    "    # to the highest expected utility (first element)\n",
    "    policy[s] = best_utility[1]\n",
    "\n",
    "# upon exit, policy has the optimal policy for each state\n",
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
